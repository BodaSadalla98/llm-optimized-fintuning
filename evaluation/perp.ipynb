{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, LlamaTokenizer, BitsAndBytesConfig\n",
    "from peft import PeftModel \n",
    "from args_parser import get_args\n",
    "from utils import *\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(model='english', model_name='/home/daria.kotova/jais', use_flash_attention_2=False, report_to='tensorboard', save_steps=400, max_seq_length=512, checkpoint_path='experiments/jais/checkpoint-41000', do_eval=False, do_train=False, evaluation_strategy='steps', eval_steps=10, logging_dir='./logs', logging_steps=100, log_level='info', logging_strategy='steps', save_total_limit=10, run_name='Mistral', base_prompt='Below is a story idea. Write a short story based on this context.', train_dataset_source_path='datasets/english/writingPrompts/train.wp_source', train_dataset_target_path='datasets/english/writingPrompts/train.wp_target', val_dataset_source_path='datasets/english/writingPrompts/valid.wp_source', val_dataset_target_path='datasets/english/writingPrompts/valid.wp_target', test_dataset_source_path='datasets/english/writingPrompts/test.wp_source', test_dataset_target_path='datasets/english/writingPrompts/test.wp_target', source_path='datasets/english/writingPrompts/test.wp_source', target_path='datasets/english/writingPrompts/test.wp_target', field='prompt', per_device_train_batch_size=4, per_device_val_batch_size=1, output_dir='./experiments', max_steps=10000, gradient_accumulation_steps=2, optim='paged_adamw_32bit', learning_rate=0.0002, max_grad_norm=0.3, warmup_ratio=0.03, lr_scheduler_type='constant', group_by_length=True, bnb_4bit_quant_type='nf4', bnb_4bit_compute_dtype='bfloat16', bnb_4bit_use_double_quant=True, gradient_checkpointing=False, lora_alpha=16, lora_dropout=0.1, lora_r=64, bias='none', task_type='CAUSAL_LM', lora_target_modules=['q_proj', 'up_proj', 'o_proj', 'k_proj', 'down_proj', 'gate_proj', 'v_proj'])\n",
      "Found cached dataset parquet (/home/daria.kotova/.cache/huggingface/datasets/arbml___parquet/arbml--Ashaar_dataset-afb4c3e63f1ca10a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/home/daria.kotova/.cache/huggingface/datasets/arbml___parquet/arbml--Ashaar_dataset-afb4c3e63f1ca10a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Found cached dataset parquet (/home/daria.kotova/.cache/huggingface/datasets/arbml___parquet/arbml--Ashaar_dataset-afb4c3e63f1ca10a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Loading cached processed dataset at /home/daria.kotova/.cache/huggingface/datasets/arbml___parquet/arbml--Ashaar_dataset-afb4c3e63f1ca10a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-9f14ce0988b36791.arrow\n",
      "Loading cached processed dataset at /home/daria.kotova/.cache/huggingface/datasets/arbml___parquet/arbml--Ashaar_dataset-afb4c3e63f1ca10a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-f4eba22cf1babc5c.arrow\n",
      "Loading cached processed dataset at /home/daria.kotova/.cache/huggingface/datasets/arbml___parquet/arbml--Ashaar_dataset-afb4c3e63f1ca10a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-ad71bc1a4d9b5b17.arrow\n",
      "1. Loaded dataset.\n",
      "Loading checkpoint shards: 100%|██████████████████| 6/6 [07:07<00:00, 71.33s/it]\n",
      "2. Successfully loaded the model /home/daria.kotova/jais into memory\n",
      "Saving the model to ./experiments/jais\n",
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]out logits:  tensor([[[ 2.7148,  7.0352,  7.9570,  ..., -5.4961, -2.8828, -0.5781],\n",
      "         [ 5.1055,  5.1875,  3.6465,  ..., -3.4746, -1.9414, -1.2422],\n",
      "         [ 9.3516,  9.4297,  5.9609,  ..., -4.0781,  4.9180, -0.7788],\n",
      "         ...,\n",
      "         [ 5.6211,  4.1758,  5.2500,  ..., -2.3359, -1.7051, -6.5781],\n",
      "         [ 4.4805,  2.3770,  4.6055,  ..., -3.2988, -4.3750, -3.9961],\n",
      "         [11.3906,  8.1328,  7.3633,  ..., -2.1504, -0.1854, -3.6289]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.6758, device='cuda:0', dtype=torch.float16)\n",
      "  1%|▍                                          | 1/100 [00:02<04:55,  2.98s/it]out logits:  tensor([[[ 2.7148,  7.0352,  7.9570,  ..., -5.4961, -2.8828, -0.5781],\n",
      "         [ 5.1055,  5.1875,  3.6465,  ..., -3.4746, -1.9414, -1.2422],\n",
      "         [ 9.3516,  9.4297,  5.9609,  ..., -4.0781,  4.9180, -0.7788],\n",
      "         ...,\n",
      "         [ 0.5273,  2.4121,  3.0020,  ..., -2.4746, -4.9805, -2.4062],\n",
      "         [ 6.5391,  5.2734,  3.2949,  ..., -3.9961, -2.2480, -2.6055],\n",
      "         [10.1641,  7.1133,  6.1914,  ..., -3.0195, -0.4771, -3.8301]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.8691, device='cuda:0', dtype=torch.float16)\n",
      "  2%|▊                                          | 2/100 [00:04<03:25,  2.10s/it]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 3.3223,  2.8086,  3.4785,  ..., -2.9629, -3.5449, -2.5137],\n",
      "         [ 5.6016,  3.3008,  3.3184,  ..., -3.8965, -2.3828, -0.9438],\n",
      "         [11.3125,  8.8047,  6.1250,  ..., -2.1582, -0.5723, -3.6855]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.8594, device='cuda:0', dtype=torch.float16)\n",
      "  3%|█▎                                         | 3/100 [00:05<02:34,  1.60s/it]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 6.7070,  2.4648,  3.5293,  ..., -4.8945, -2.0176, -8.5938],\n",
      "         [ 4.0312,  1.7803,  3.7930,  ..., -2.0410, -3.2129, -5.7109],\n",
      "         [10.1797,  7.1484,  5.9336,  ..., -2.4863, -0.8203, -5.9258]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.9512, device='cuda:0', dtype=torch.float16)\n",
      "  4%|█▋                                         | 4/100 [00:06<02:11,  1.37s/it]out logits:  tensor([[[ 2.6992,  7.0039,  7.9453,  ..., -5.5078, -2.8984, -0.6045],\n",
      "         [ 5.1016,  5.1641,  3.6230,  ..., -3.5020, -1.9551, -1.2637],\n",
      "         [ 9.3906,  9.4297,  5.9727,  ..., -4.0977,  4.9141, -0.7705],\n",
      "         ...,\n",
      "         [ 7.5508,  4.7969,  4.0430,  ..., -2.0000, -0.5908, -1.3486],\n",
      "         [ 5.0938,  2.7422,  3.1426,  ..., -1.7988, -3.7461, -2.9141],\n",
      "         [ 9.6328,  8.5078,  5.6523,  ..., -2.7188, -0.5854, -3.1621]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(2.4355, device='cuda:0', dtype=torch.float16)\n",
      "  5%|██▏                                        | 5/100 [00:07<01:48,  1.14s/it]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 8.6953,  5.1875,  4.6523,  ..., -3.7441, -0.0490, -2.1934],\n",
      "         [ 6.4375,  3.4121,  4.2031,  ..., -4.8867, -2.4180, -2.5156],\n",
      "         [10.8281,  7.4023,  6.0547,  ..., -1.4385, -0.6548, -2.7734]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.9199, device='cuda:0', dtype=torch.float16)\n",
      "  6%|██▌                                        | 6/100 [00:08<01:44,  1.11s/it]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 6.2188,  4.2031,  5.1445,  ..., -3.8789, -1.1748,  1.4404],\n",
      "         [ 3.7402,  3.1953,  3.9336,  ..., -5.8984, -4.2344, -1.4004],\n",
      "         [12.4531,  8.0547,  6.8438,  ..., -2.5996, -1.3320, -3.2773]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.9297, device='cuda:0', dtype=torch.float16)\n",
      "  7%|███                                        | 7/100 [00:09<01:47,  1.15s/it]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 5.3672,  3.6250,  3.9062,  ..., -3.5410, -5.1016, -4.1562],\n",
      "         [10.2656,  7.0859,  5.0781,  ..., -2.2832, -3.0449, -3.2969],\n",
      "         [ 9.7734,  8.1328,  6.0195,  ..., -2.2109, -1.7939, -3.9902]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.7930, device='cuda:0', dtype=torch.float16)\n",
      "  8%|███▍                                       | 8/100 [00:10<01:49,  1.19s/it]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 6.0547,  4.3164,  7.1836,  ..., -3.2188, -2.0879, -4.8398],\n",
      "         [ 9.9688,  5.9531,  5.6367,  ..., -4.6055, -1.3682, -5.1875],\n",
      "         [10.5938,  6.5781,  6.0820,  ..., -2.2324, -0.7822, -3.1914]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.7969, device='cuda:0', dtype=torch.float16)\n",
      "  9%|███▊                                       | 9/100 [00:11<01:42,  1.13s/it]out logits:  tensor([[[ 2.7148,  7.0352,  7.9570,  ..., -5.4961, -2.8828, -0.5781],\n",
      "         [ 5.1055,  5.1875,  3.6465,  ..., -3.4746, -1.9414, -1.2422],\n",
      "         [ 9.3516,  9.4297,  5.9609,  ..., -4.0781,  4.9180, -0.7788],\n",
      "         ...,\n",
      "         [ 8.6250,  5.9297,  5.0742,  ..., -2.8262, -4.1719, -4.1914],\n",
      "         [ 4.4688,  3.1289,  2.9316,  ..., -3.7012, -4.5469, -5.3945],\n",
      "         [13.2344,  7.7930,  7.5547,  ..., -1.7500, -0.8237, -3.9316]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(2.9004, device='cuda:0', dtype=torch.float16)\n",
      " 10%|████▏                                     | 10/100 [00:13<01:50,  1.23s/it]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 8.2188,  4.2344,  5.2070,  ..., -2.0098, -2.5859, -5.8164],\n",
      "         [ 8.4609,  4.6406,  5.3711,  ..., -1.7188, -2.6348, -6.6914],\n",
      "         [10.9297,  7.8750,  6.9609,  ..., -2.1484, -1.6641, -3.5625]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.8398, device='cuda:0', dtype=torch.float16)\n",
      " 11%|████▌                                     | 11/100 [00:14<01:43,  1.17s/it]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 5.4492,  2.9316,  3.6914,  ..., -2.2266, -4.4297, -3.9141],\n",
      "         [ 4.5625,  3.0469,  4.6016,  ..., -3.5332, -4.1680, -4.7266],\n",
      "         [10.5000,  6.1250,  6.2539,  ..., -1.6738, -1.9482, -4.4961]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.7285, device='cuda:0', dtype=torch.float16)\n",
      " 12%|█████                                     | 12/100 [00:15<01:38,  1.11s/it]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 3.3887,  2.8574,  4.7305,  ..., -2.8633, -2.7812, -3.6250],\n",
      "         [ 7.3125,  5.0195,  4.8555,  ..., -3.4551, -1.0791, -2.6816],\n",
      "         [11.1953,  6.4648,  6.1523,  ..., -2.6523, -0.3591, -4.9180]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.9629, device='cuda:0', dtype=torch.float16)\n",
      " 13%|█████▍                                    | 13/100 [00:16<01:41,  1.17s/it]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 6.8789,  3.8711,  3.7227,  ..., -6.0898, -3.9316, -6.0508],\n",
      "         [ 6.5820,  4.2969,  5.2461,  ..., -1.3252, -4.6406, -8.1641],\n",
      "         [11.0156,  8.2812,  6.8203,  ..., -1.9082, -0.2812, -3.8027]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.8965, device='cuda:0', dtype=torch.float16)\n",
      " 14%|█████▉                                    | 14/100 [00:17<01:29,  1.05s/it]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 3.3008,  3.0547,  4.5156,  ..., -2.0059, -2.6582, -3.4570],\n",
      "         [ 6.7617,  7.3398,  6.0938,  ..., -2.0234, -1.8594, -2.1953],\n",
      "         [12.1328, 10.6484,  6.8008,  ..., -2.0801, -2.8770, -4.9531]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(4.1133, device='cuda:0', dtype=torch.float16)\n",
      " 15%|██████▎                                   | 15/100 [00:18<01:35,  1.12s/it]out logits:  tensor([[[ 2.7148,  7.0352,  7.9570,  ..., -5.4961, -2.8828, -0.5781],\n",
      "         [ 5.1055,  5.1875,  3.6465,  ..., -3.4746, -1.9414, -1.2422],\n",
      "         [ 9.3516,  9.4297,  5.9609,  ..., -4.0781,  4.9180, -0.7788],\n",
      "         ...,\n",
      "         [ 4.6094,  2.5430,  3.1191,  ..., -1.7812, -2.6309, -2.0566],\n",
      "         [ 4.4922,  2.5078,  2.7852,  ..., -3.7715, -3.2891, -2.6543],\n",
      "         [10.3906,  8.3281,  6.0234,  ..., -2.2441, -0.4509, -3.2871]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(4.0039, device='cuda:0', dtype=torch.float16)\n",
      " 16%|██████▋                                   | 16/100 [00:19<01:37,  1.16s/it]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 8.6250,  5.0547,  4.3828,  ..., -1.1553, -2.2227, -5.6953],\n",
      "         [10.3984,  9.6250,  6.5000,  ..., -3.6328, -1.4268, -4.1680],\n",
      "         [10.9844, 10.3438,  7.7578,  ..., -2.1680, -0.5718, -3.6699]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.7910, device='cuda:0', dtype=torch.float16)\n",
      " 17%|███████▏                                  | 17/100 [00:20<01:33,  1.13s/it]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 7.2578,  8.0703,  4.5273,  ..., -1.2852, -1.3887, -4.5469],\n",
      "         [ 6.5742,  3.4980,  5.3633,  ..., -3.3379, -4.3125, -3.9688],\n",
      "         [12.2266, 10.0859,  7.1523,  ..., -1.9434, -1.6504, -3.8691]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.3555, device='cuda:0', dtype=torch.float16)\n",
      " 18%|███████▌                                  | 18/100 [00:21<01:29,  1.09s/it]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 8.2188,  5.1602,  5.7188,  ..., -4.9492, -2.1738, -5.1758],\n",
      "         [ 8.2578,  6.3320,  5.4844,  ..., -4.9180, -3.6914, -3.6289],\n",
      "         [11.7266,  8.3594,  5.9961,  ..., -3.0430, -1.6299, -3.5312]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(4.2070, device='cuda:0', dtype=torch.float16)\n",
      " 19%|███████▉                                  | 19/100 [00:22<01:20,  1.00it/s]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 3.5137,  3.8516,  3.9727,  ..., -1.6924, -1.6396, -2.3711],\n",
      "         [ 5.2812,  4.2930,  3.9766,  ..., -1.9443, -1.5146, -4.4258],\n",
      "         [ 9.5234,  8.4297,  5.0312,  ..., -0.9585, -0.7036, -3.8711]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.3750, device='cuda:0', dtype=torch.float16)\n",
      " 20%|████████▍                                 | 20/100 [00:23<01:20,  1.01s/it]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 4.3398,  5.1172,  5.1406,  ..., -1.8564, -1.3262, -3.3125],\n",
      "         [ 1.7236,  3.9492,  3.9004,  ..., -3.3555, -2.9238, -2.8711],\n",
      "         [11.7031,  7.8398,  6.2773,  ..., -2.7344, -0.9561, -3.1191]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.7422, device='cuda:0', dtype=torch.float16)\n",
      " 21%|████████▊                                 | 21/100 [00:25<01:26,  1.10s/it]out logits:  tensor([[[ 2.7148,  7.0352,  7.9570,  ..., -5.4961, -2.8828, -0.5781],\n",
      "         [ 5.1055,  5.1875,  3.6465,  ..., -3.4746, -1.9414, -1.2422],\n",
      "         [ 9.3516,  9.4297,  5.9609,  ..., -4.0781,  4.9180, -0.7788],\n",
      "         ...,\n",
      "         [ 8.1484,  4.9688,  5.7734,  ..., -4.1055, -2.5078, -3.3633],\n",
      "         [ 5.6328,  5.0742,  5.2305,  ..., -4.4180, -3.5156, -6.5664],\n",
      "         [12.0000,  8.6250,  5.8906,  ..., -2.0918,  0.0598, -3.4062]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.6562, device='cuda:0', dtype=torch.float16)\n",
      " 22%|█████████▏                                | 22/100 [00:26<01:45,  1.35s/it]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.1055,  5.2109,  3.6562,  ..., -3.4531, -1.9414, -1.2266],\n",
      "         [ 9.3438,  9.4062,  5.9570,  ..., -4.0625,  4.8633, -0.7852],\n",
      "         ...,\n",
      "         [ 5.8906,  5.8047,  6.2617,  ..., -3.7402, -2.4531, -1.6846],\n",
      "         [ 5.0156,  4.4609,  5.3125,  ..., -5.3633, -4.3594, -4.3125],\n",
      "         [11.6094,  7.4727,  5.7305,  ..., -2.9023, -1.8975, -3.7012]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.6133, device='cuda:0', dtype=torch.float16)\n",
      " 23%|█████████▋                                | 23/100 [00:28<01:36,  1.26s/it]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 5.4219,  4.0078,  4.2148,  ..., -4.7383, -0.4038, -1.2529],\n",
      "         [ 7.5352,  4.5781,  4.6445,  ..., -5.6602, -2.5723, -1.0127],\n",
      "         [12.3750,  8.1094,  7.1211,  ..., -3.1270, -0.4775, -2.8164]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.7168, device='cuda:0', dtype=torch.float16)\n",
      " 24%|██████████                                | 24/100 [00:29<01:30,  1.19s/it]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 6.2500,  3.4980,  4.4609,  ..., -1.9951, -1.9727, -4.7969],\n",
      "         [ 8.8594,  6.0156,  5.1680,  ..., -3.3730, -2.9395, -5.2695],\n",
      "         [10.5391,  7.5977,  5.7305,  ..., -2.3828, -1.2520, -3.8750]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(4.6875, device='cuda:0', dtype=torch.float16)\n",
      " 25%|██████████▌                               | 25/100 [00:30<01:25,  1.14s/it]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 5.8555,  3.6387,  5.6094,  ..., -5.3438, -2.1758, -6.5703],\n",
      "         [ 5.4453,  4.6094,  5.1992,  ..., -1.2637, -1.4287, -5.3359],\n",
      "         [ 9.9609,  8.0469,  6.9883,  ..., -0.8691, -0.4229, -4.7188]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.5742, device='cuda:0', dtype=torch.float16)\n",
      " 26%|██████████▉                               | 26/100 [00:31<01:21,  1.10s/it]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 5.4609,  4.7305,  5.3008,  ..., -3.5840, -2.9082, -2.8320],\n",
      "         [10.4844,  6.8945,  5.6094,  ..., -3.6641, -0.9531, -4.9766],\n",
      "         [10.8203,  7.5898,  6.5898,  ..., -3.8867,  0.1136, -4.6914]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.8086, device='cuda:0', dtype=torch.float16)\n",
      " 27%|███████████▎                              | 27/100 [00:32<01:19,  1.09s/it]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 5.8789,  3.9961,  5.4805,  ..., -3.6562, -1.8701, -2.7168],\n",
      "         [ 6.3672,  2.7344,  5.0156,  ..., -2.7773, -4.0352, -4.5430],\n",
      "         [10.8203,  9.0078,  6.1602,  ..., -2.3027, -0.6465, -3.7402]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.5527, device='cuda:0', dtype=torch.float16)\n",
      " 28%|███████████▊                              | 28/100 [00:33<01:17,  1.07s/it]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 5.1406,  4.5156,  4.9141,  ..., -0.7275, -1.3281, -2.8047],\n",
      "         [ 4.0508,  3.1660,  6.0586,  ..., -1.8682, -4.2266, -3.7715],\n",
      "         [11.7578,  7.7031,  7.9453,  ..., -2.2891, -0.5913, -4.0781]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.5703, device='cuda:0', dtype=torch.float16)\n",
      " 29%|████████████▏                             | 29/100 [00:34<01:16,  1.07s/it]out logits:  tensor([[[ 2.6992,  7.0039,  7.9453,  ..., -5.5078, -2.8984, -0.6045],\n",
      "         [ 5.1016,  5.1641,  3.6230,  ..., -3.5020, -1.9551, -1.2637],\n",
      "         [ 9.3906,  9.4297,  5.9727,  ..., -4.0977,  4.9141, -0.7705],\n",
      "         ...,\n",
      "         [ 2.7715,  3.2852,  3.7617,  ..., -3.7500, -1.9600, -2.4023],\n",
      "         [ 3.4297,  4.0938,  5.4414,  ..., -4.1211, -0.7969, -3.0742],\n",
      "         [ 9.0625,  7.1523,  5.4062,  ..., -2.7422, -0.8135, -3.4473]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(2.6562, device='cuda:0', dtype=torch.float16)\n",
      " 30%|████████████▌                             | 30/100 [00:35<01:08,  1.02it/s]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 6.3242,  4.4023,  6.7461,  ..., -4.3867, -1.8906, -4.9492],\n",
      "         [ 9.3984,  7.6328,  6.9219,  ..., -2.8047, -2.0469, -3.6562],\n",
      "         [ 9.9609,  6.9648,  4.8828,  ..., -2.5488, -2.3965, -3.5156]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.4551, device='cuda:0', dtype=torch.float16)\n",
      " 31%|█████████████                             | 31/100 [00:35<01:04,  1.07it/s]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.1055,  5.2109,  3.6562,  ..., -3.4531, -1.9414, -1.2266],\n",
      "         [ 9.3438,  9.4062,  5.9570,  ..., -4.0625,  4.8633, -0.7852],\n",
      "         ...,\n",
      "         [ 1.1592,  2.5918,  1.4912,  ..., -2.9648, -3.2500, -1.5918],\n",
      "         [ 6.8242,  6.3672,  5.0586,  ..., -5.8242,  0.1876,  0.8892],\n",
      "         [ 9.7266,  7.1758,  5.9258,  ..., -3.6152, -2.0684, -3.9648]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.1641, device='cuda:0', dtype=torch.float16)\n",
      " 32%|█████████████▍                            | 32/100 [00:36<01:05,  1.04it/s]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 2.3184,  0.8374,  3.1270,  ..., -4.6094, -3.7148, -3.3555],\n",
      "         [ 7.3125,  2.1797,  3.8574,  ..., -5.0234, -2.9414, -1.9482],\n",
      "         [10.0234,  5.8789,  6.0039,  ..., -2.6406, -1.4268, -0.6572]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.9922, device='cuda:0', dtype=torch.float16)\n",
      " 33%|█████████████▊                            | 33/100 [00:37<01:06,  1.01it/s]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 6.7812,  4.8984,  4.5469,  ..., -2.7637, -1.8906, -1.0498],\n",
      "         [ 6.6562,  3.9941,  5.6367,  ..., -4.1758, -3.0957, -2.7676],\n",
      "         [11.9688,  9.6406,  7.2461,  ..., -2.7715, -0.8413, -1.0244]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(4.2383, device='cuda:0', dtype=torch.float16)\n",
      " 34%|██████████████▎                           | 34/100 [00:38<01:05,  1.00it/s]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 4.0508,  1.9062,  3.8379,  ..., -3.6074, -1.5928, -3.6152],\n",
      "         [ 5.6836,  3.2871,  3.3906,  ..., -2.5723, -3.7500, -5.7383],\n",
      "         [ 8.1094,  6.3945,  5.3984,  ..., -3.9453, -1.0322, -3.3086]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(4.6914, device='cuda:0', dtype=torch.float16)\n",
      " 35%|██████████████▋                           | 35/100 [00:39<01:05,  1.00s/it]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 3.9727,  1.3291,  3.4648,  ..., -4.6250, -1.6543, -3.0020],\n",
      "         [ 6.0977,  3.5156,  4.1094,  ..., -4.9492, -2.6660, -1.8008],\n",
      "         [ 8.5859,  3.9062,  4.1719,  ..., -3.1562, -0.3757, -1.7959]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(4.6250, device='cuda:0', dtype=torch.float16)\n",
      " 36%|███████████████                           | 36/100 [00:41<01:05,  1.02s/it]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 9.6172,  5.8008,  3.7441,  ..., -4.8125, -1.9531, -2.6504],\n",
      "         [ 5.6211,  3.2500,  2.1797,  ..., -4.3203, -4.3906, -7.4023],\n",
      "         [10.1797,  7.7109,  5.5156,  ..., -2.1797, -2.2461, -4.6641]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.4707, device='cuda:0', dtype=torch.float16)\n",
      " 37%|███████████████▌                          | 37/100 [00:42<01:04,  1.03s/it]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [-0.2041,  0.5762,  1.9414,  ..., -2.2734, -4.1836, -5.9570],\n",
      "         [ 6.1875,  5.1328,  5.5312,  ..., -1.4043, -4.3086, -7.0078],\n",
      "         [10.5156,  5.7500,  5.8672,  ..., -2.4785, -2.2148, -4.0977]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.5859, device='cuda:0', dtype=torch.float16)\n",
      " 38%|███████████████▉                          | 38/100 [00:43<01:08,  1.11s/it]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 4.6758,  5.1875,  3.7051,  ..., -3.4883, -3.1348, -3.9102],\n",
      "         [ 2.6113,  4.0312,  4.0195,  ..., -6.3516, -1.6006, -3.3164],\n",
      "         [ 7.7656,  5.6641,  4.5430,  ..., -4.0820, -1.5820, -4.8711]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.0703, device='cuda:0', dtype=torch.float16)\n",
      " 39%|████████████████▍                         | 39/100 [00:44<01:02,  1.02s/it]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 3.6562,  2.7539,  5.4531,  ..., -2.1152, -1.7598, -3.0781],\n",
      "         [10.0859,  6.1328,  5.4453,  ..., -3.4941, -2.8770, -4.2188],\n",
      "         [ 9.8125,  7.1211,  4.9492,  ..., -2.6562, -2.2070, -3.9238]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.8398, device='cuda:0', dtype=torch.float16)\n",
      " 40%|████████████████▊                         | 40/100 [00:45<01:01,  1.03s/it]out logits:  tensor([[[ 2.7070,  7.0039,  7.9258,  ..., -5.4922, -2.9102, -0.5708],\n",
      "         [ 5.1172,  5.1914,  3.6641,  ..., -3.4922, -1.9746, -1.2441],\n",
      "         [ 9.3750,  9.4219,  6.0039,  ..., -4.0820,  4.9023, -0.7686],\n",
      "         ...,\n",
      "         [ 4.4609,  4.5938,  4.8906,  ..., -4.8711, -2.7695, -2.0605],\n",
      "         [ 0.5039,  1.0918,  2.7988,  ..., -3.8574, -3.2383, -4.1562],\n",
      "         [11.3281,  5.2773,  5.6250,  ..., -3.6016, -2.9238, -4.9961]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.8672, device='cuda:0', dtype=torch.float16)\n",
      " 41%|█████████████████▏                        | 41/100 [00:47<01:26,  1.46s/it]out logits:  tensor([[[ 2.7266,  6.9766,  7.9375,  ..., -5.4961, -2.9102, -0.6367],\n",
      "         [ 5.0898,  5.1758,  3.6367,  ..., -3.4746, -1.9570, -1.2568],\n",
      "         [ 9.3906,  9.4453,  5.9648,  ..., -4.0625,  4.8867, -0.7690],\n",
      "         ...,\n",
      "         [ 1.7676,  1.4551,  4.0625,  ..., -4.2578, -2.8730, -3.8574],\n",
      "         [ 3.2305,  1.7285,  3.8926,  ..., -4.6680, -1.7383, -4.0898],\n",
      "         [11.3984,  5.8164,  5.7461,  ..., -3.3984, -3.2812, -4.2656]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(3.0645, device='cuda:0', dtype=torch.float16)\n",
      " 42%|█████████████████▋                        | 42/100 [00:48<01:18,  1.35s/it]out logits:  tensor([[[ 2.7070,  7.0039,  7.9258,  ..., -5.4922, -2.9102, -0.5708],\n",
      "         [ 5.1172,  5.1914,  3.6641,  ..., -3.4922, -1.9746, -1.2441],\n",
      "         [ 9.3750,  9.4219,  6.0039,  ..., -4.0820,  4.9023, -0.7686],\n",
      "         ...,\n",
      "         [ 3.5098,  1.2236,  2.0547,  ..., -4.1211, -0.1422, -1.6328],\n",
      "         [ 1.9150,  2.1660,  3.9863,  ..., -5.2344, -2.0840, -3.1621],\n",
      "         [10.9531,  5.2109,  4.8008,  ..., -3.0332, -2.2266, -3.5039]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(4.2969, device='cuda:0', dtype=torch.float16)\n",
      " 43%|██████████████████                        | 43/100 [00:51<01:36,  1.69s/it]out logits:  tensor([[[ 2.7148,  7.0352,  7.9570,  ..., -5.4961, -2.8828, -0.5781],\n",
      "         [ 5.1055,  5.1875,  3.6465,  ..., -3.4746, -1.9414, -1.2422],\n",
      "         [ 9.3516,  9.4297,  5.9609,  ..., -4.0781,  4.9180, -0.7788],\n",
      "         ...,\n",
      "         [ 4.9453,  3.1758,  4.6172,  ...,  0.3987, -3.7129, -2.6973],\n",
      "         [ 6.9336,  5.7188,  6.0312,  ..., -2.8281, -2.6250, -4.7539],\n",
      "         [10.4688,  7.8828,  4.6680,  ..., -0.9019, -2.5625, -3.8301]]],\n",
      "       device='cuda:0', dtype=torch.float16)\n",
      "loss:  tensor(4.6484, device='cuda:0', dtype=torch.float16)\n",
      " 44%|██████████████████▍                       | 44/100 [00:52<01:32,  1.64s/it]^C\n",
      " 44%|██████████████████▍                       | 44/100 [00:56<01:11,  1.28s/it]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/daria.kotova/ai/llm-optimized-fintuning/calculate_perplexity.py\", line 81, in <module>\n",
      "    out_logits = model(**inputs).logits\n",
      "                 ^^^^^^^^^^^^^^^\n",
      "  File \"/home/daria.kotova/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/daria.kotova/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/daria.kotova/miniconda3/lib/python3.11/site-packages/peft/peft_model.py\", line 977, in forward\n",
      "    return self.base_model(\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/daria.kotova/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/daria.kotova/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/daria.kotova/miniconda3/lib/python3.11/site-packages/peft/tuners/tuners_utils.py\", line 106, in forward\n",
      "    return self.model.forward(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/daria.kotova/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 164, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/daria.kotova/.cache/huggingface/modules/transformers_modules/jais/modeling_jais.py\", line 1115, in forward\n",
      "    transformer_outputs = self.transformer(\n",
      "                          ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/daria.kotova/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/daria.kotova/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/daria.kotova/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 164, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/daria.kotova/.cache/huggingface/modules/transformers_modules/jais/modeling_jais.py\", line 936, in forward\n",
      "    outputs = block(\n",
      "              ^^^^^^\n",
      "  File \"/home/daria.kotova/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/daria.kotova/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/daria.kotova/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 164, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/daria.kotova/.cache/huggingface/modules/transformers_modules/jais/modeling_jais.py\", line 441, in forward\n",
      "    attn_outputs = self.attn(\n",
      "                   ^^^^^^^^^^\n",
      "  File \"/home/daria.kotova/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/daria.kotova/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/daria.kotova/miniconda3/lib/python3.11/site-packages/accelerate/hooks.py\", line 164, in new_forward\n",
      "    output = module._old_forward(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/daria.kotova/.cache/huggingface/modules/transformers_modules/jais/modeling_jais.py\", line 377, in forward\n",
      "    attn_output, attn_weights = self._attn(query, key, value, attention_mask, head_mask, position_bias)\n",
      "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/daria.kotova/.cache/huggingface/modules/transformers_modules/jais/modeling_jais.py\", line 242, in _attn\n",
      "    mask_value = torch.full([], mask_value, dtype=attn_weights.dtype).to(attn_weights.device)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python calculate_perplexity.py --checkpoint_path=\"experiments/jais/checkpoint-41000\" \\\n",
    "--per_device_val_batch_size=1 --model_name='/home/daria.kotova/jais'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset text (/home/daria.kotova/.cache/huggingface/datasets/text/default-ef683339c6e1d1b5/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Found cached dataset text (/home/daria.kotova/.cache/huggingface/datasets/text/default-594e1b1157c143e0/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
      "Loading cached processed dataset at /home/daria.kotova/.cache/huggingface/datasets/text/default-ef683339c6e1d1b5/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-f74c4750f7345492.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bb96b7d90e34c3bbaeb3b8d28acf1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp=0.9\n",
    "top_p=0.6\n",
    "max_length=1024\n",
    "\n",
    "args = get_args()\n",
    "dataset = get_dataset(\n",
    "        args.source_path, args.target_path, field=\"prompt\", prompt_only=True\n",
    "    )\n",
    "    \n",
    "\n",
    "model_name = args.model_name\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    # bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    load_in_4bit=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='auto'\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, \"experiments/Llama-2-7b-hf/checkpoint-10000\")\n",
    "# m = m.merge_and_unload()\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_name)\n",
    "tokenizer.bos_token_id = 1\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=1 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15138\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 23/15138 [00:14<2:34:00,  1.64it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m      7\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(dataloader):\n\u001b[0;32m----> 8\u001b[0m         inputs \u001b[39m=\u001b[39m tokenizer(batch[\u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m], padding\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m         story_lengths\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(tokenizer(batch[\u001b[39m\"\u001b[39m\u001b[39mstory\u001b[39m\u001b[39m\"\u001b[39m], padding\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m])\n\u001b[1;32m     12\u001b[0m         out_logits \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\u001b[39m.\u001b[39mlogits\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:789\u001b[0m, in \u001b[0;36mBatchEncoding.to\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[39m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[39m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    787\u001b[0m \u001b[39m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(device, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m is_torch_device(device) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(device, \u001b[39mint\u001b[39m):\n\u001b[0;32m--> 789\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m    790\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    791\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(device)\u001b[39m}\u001b[39;00m\u001b[39m. This is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:789\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[39m# This check catches things like APEX blindly calling \"to\" on all inputs to a module\u001b[39;00m\n\u001b[1;32m    786\u001b[0m \u001b[39m# Otherwise it passes the casts down and casts the LongTensor containing the token idxs\u001b[39;00m\n\u001b[1;32m    787\u001b[0m \u001b[39m# into a HalfTensor\u001b[39;00m\n\u001b[1;32m    788\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(device, \u001b[39mstr\u001b[39m) \u001b[39mor\u001b[39;00m is_torch_device(device) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(device, \u001b[39mint\u001b[39m):\n\u001b[0;32m--> 789\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m {k: v\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m    790\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    791\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttempting to cast a BatchEncoding to type \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mstr\u001b[39m(device)\u001b[39m}\u001b[39;00m\u001b[39m. This is not supported.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pad_id  = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "\n",
    "loss_fct = CrossEntropyLoss(reduction=\"mean\", ignore_index=pad_id)\n",
    "perplexities = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dataloader):\n",
    "        inputs = tokenizer(batch[\"prompt\"], padding=False, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "        story_lengths= len(tokenizer(batch[\"story\"], padding=False, return_tensors=\"pt\")['input_ids'][0])\n",
    "\n",
    "        out_logits = model(**inputs).logits\n",
    "        out_logits =  out_logits[0][-story_lengths:]\n",
    "        labels_inputs = inputs['input_ids'][0][-story_lengths:]\n",
    "        shifted_labels = labels_inputs[1:]\n",
    "        shifted_logits = out_logits[:-1]\n",
    "\n",
    "        perplexities.append(torch.exp(loss_fct(shifted_logits, shifted_labels)))\n",
    "\n",
    "        # nll = loss_fct(out_logits , labels_inputs)\n",
    "        # perplexity_batch = torch.exp(nll)\n",
    "\n",
    "        # nlls.append(nll)\n",
    "\n",
    "        # outputs = model(inputs[\"input_ids\"], labels=inputs[\"input_ids\"])\n",
    "        # neg_log_likelihood = outputs.loss\n",
    "        # # print(nll, neg_log_likelihood)\n",
    "\n",
    "        # perplexities_model.append(torch.exp(neg_log_likelihood).item())\n",
    "        # perplexities_loss.append(perplexity_batch.item())\n",
    "\n",
    "        \n",
    "\n",
    "    ppl_model = np.mean(perplexities)\n",
    "    # ppl_loss = np.mean(perplexities_loss)\n",
    "    print(\"Final perplexity is: \", ppl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [[1, 0, 0, 0, 0],\n",
    "     [0, 0, 1, 0, 0],\n",
    "     [0, 0, 1, 0, 0],\n",
    "     [0, 0, 0, 1, 0]]\n",
    "b = [0, 2, 2, 3]\n",
    "\n",
    "loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "a = torch.tensor(np.array(a))\n",
    "b = torch.tensor(np.array(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.4715, 2.4715, 2.4715, 2.4715])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(loss_fct(a.float(), b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9048, 0.9048, 0.9048, 0.9048])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fct(a.float(), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/IPython/core/formatters.py:708\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    701\u001b[0m stream \u001b[39m=\u001b[39m StringIO()\n\u001b[1;32m    702\u001b[0m printer \u001b[39m=\u001b[39m pretty\u001b[39m.\u001b[39mRepresentationPrinter(stream, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m    703\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_width, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnewline,\n\u001b[1;32m    704\u001b[0m     max_seq_length\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_seq_length,\n\u001b[1;32m    705\u001b[0m     singleton_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msingleton_printers,\n\u001b[1;32m    706\u001b[0m     type_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtype_printers,\n\u001b[1;32m    707\u001b[0m     deferred_pprinters\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdeferred_printers)\n\u001b[0;32m--> 708\u001b[0m printer\u001b[39m.\u001b[39mpretty(obj)\n\u001b[1;32m    709\u001b[0m printer\u001b[39m.\u001b[39mflush()\n\u001b[1;32m    710\u001b[0m \u001b[39mreturn\u001b[39;00m stream\u001b[39m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/IPython/lib/pretty.py:410\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    407\u001b[0m                         \u001b[39mreturn\u001b[39;00m meth(obj, \u001b[39mself\u001b[39m, cycle)\n\u001b[1;32m    408\u001b[0m                 \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mobject\u001b[39m \\\n\u001b[1;32m    409\u001b[0m                         \u001b[39mand\u001b[39;00m \u001b[39mcallable\u001b[39m(\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39m__repr__\u001b[39m\u001b[39m'\u001b[39m)):\n\u001b[0;32m--> 410\u001b[0m                     \u001b[39mreturn\u001b[39;00m _repr_pprint(obj, \u001b[39mself\u001b[39m, cycle)\n\u001b[1;32m    412\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_pprint(obj, \u001b[39mself\u001b[39m, cycle)\n\u001b[1;32m    413\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/IPython/lib/pretty.py:778\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[1;32m    777\u001b[0m \u001b[39m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mrepr\u001b[39m(obj)\n\u001b[1;32m    779\u001b[0m lines \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39msplitlines()\n\u001b[1;32m    780\u001b[0m \u001b[39mwith\u001b[39;00m p\u001b[39m.\u001b[39mgroup():\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_tensor.py:431\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    428\u001b[0m         Tensor\u001b[39m.\u001b[39m\u001b[39m__repr__\u001b[39m, (\u001b[39mself\u001b[39m,), \u001b[39mself\u001b[39m, tensor_contents\u001b[39m=\u001b[39mtensor_contents\n\u001b[1;32m    429\u001b[0m     )\n\u001b[1;32m    430\u001b[0m \u001b[39m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_tensor_str\u001b[39m.\u001b[39m_str(\u001b[39mself\u001b[39m, tensor_contents\u001b[39m=\u001b[39mtensor_contents)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_tensor_str.py:664\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    662\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad(), torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39m_python_dispatch\u001b[39m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    663\u001b[0m     guard \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 664\u001b[0m     \u001b[39mreturn\u001b[39;00m _str_intern(\u001b[39mself\u001b[39m, tensor_contents\u001b[39m=\u001b[39mtensor_contents)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_tensor_str.py:595\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    593\u001b[0m                     tensor_str \u001b[39m=\u001b[39m _tensor_str(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    594\u001b[0m                 \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 595\u001b[0m                     tensor_str \u001b[39m=\u001b[39m _tensor_str(\u001b[39mself\u001b[39m, indent)\n\u001b[1;32m    597\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayout \u001b[39m!=\u001b[39m torch\u001b[39m.\u001b[39mstrided:\n\u001b[1;32m    598\u001b[0m     suffixes\u001b[39m.\u001b[39mappend(\u001b[39m\"\u001b[39m\u001b[39mlayout=\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayout))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_tensor_str.py:347\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[39mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    344\u001b[0m         \u001b[39mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    345\u001b[0m     )\n\u001b[1;32m    346\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 347\u001b[0m     formatter \u001b[39m=\u001b[39m _Formatter(get_summarized_data(\u001b[39mself\u001b[39m) \u001b[39mif\u001b[39;00m summarize \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m)\n\u001b[1;32m    348\u001b[0m     \u001b[39mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[39mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_tensor_str.py:383\u001b[0m, in \u001b[0;36mget_summarized_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    381\u001b[0m     start \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, PRINT_OPTS\u001b[39m.\u001b[39medgeitems)]\n\u001b[1;32m    382\u001b[0m     end \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m PRINT_OPTS\u001b[39m.\u001b[39medgeitems, \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m))]\n\u001b[0;32m--> 383\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack([get_summarized_data(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m (start \u001b[39m+\u001b[39m end)])\n\u001b[1;32m    384\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack([get_summarized_data(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_tensor_str.py:383\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    381\u001b[0m     start \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, PRINT_OPTS\u001b[39m.\u001b[39medgeitems)]\n\u001b[1;32m    382\u001b[0m     end \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m PRINT_OPTS\u001b[39m.\u001b[39medgeitems, \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m))]\n\u001b[0;32m--> 383\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack([get_summarized_data(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m (start \u001b[39m+\u001b[39m end)])\n\u001b[1;32m    384\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack([get_summarized_data(x) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.11/site-packages/torch/_tensor_str.py:373\u001b[0m, in \u001b[0;36mget_summarized_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[39mif\u001b[39;00m dim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    372\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m PRINT_OPTS\u001b[39m.\u001b[39medgeitems:\n\u001b[0;32m--> 373\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(\n\u001b[1;32m    374\u001b[0m             (\u001b[39mself\u001b[39m[: PRINT_OPTS\u001b[39m.\u001b[39medgeitems], \u001b[39mself\u001b[39m[\u001b[39m-\u001b[39mPRINT_OPTS\u001b[39m.\u001b[39medgeitems :])\n\u001b[1;32m    375\u001b[0m         )\n\u001b[1;32m    376\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    377\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "out_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
