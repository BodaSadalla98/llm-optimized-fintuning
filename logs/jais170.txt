starting.......................
llm_evaluation
Found cached dataset parquet (/home/daria.kotova/.cache/huggingface/datasets/arbml___parquet/arbml--Ashaar_dataset-afb4c3e63f1ca10a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
Found cached dataset parquet (/home/daria.kotova/.cache/huggingface/datasets/arbml___parquet/arbml--Ashaar_dataset-afb4c3e63f1ca10a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
Found cached dataset parquet (/home/daria.kotova/.cache/huggingface/datasets/arbml___parquet/arbml--Ashaar_dataset-afb4c3e63f1ca10a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)
Loading cached processed dataset at /home/daria.kotova/.cache/huggingface/datasets/arbml___parquet/arbml--Ashaar_dataset-afb4c3e63f1ca10a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-9f14ce0988b36791.arrow
Loading cached processed dataset at /home/daria.kotova/.cache/huggingface/datasets/arbml___parquet/arbml--Ashaar_dataset-afb4c3e63f1ca10a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-f4eba22cf1babc5c.arrow
Loading cached processed dataset at /home/daria.kotova/.cache/huggingface/datasets/arbml___parquet/arbml--Ashaar_dataset-afb4c3e63f1ca10a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-ad71bc1a4d9b5b17.arrow
Namespace(model='english', model_name='/home/daria.kotova/jais', use_flash_attention_2=False, report_to='tensorboard', save_steps=400, max_seq_length=512, checkpoint_path='experiments/jais/checkpoint-170000', do_eval=False, do_train=False, evaluation_strategy='steps', eval_steps=10, logging_dir='./logs', logging_steps=100, log_level='info', logging_strategy='steps', save_total_limit=10, run_name='Mistral', base_prompt='Below is a story idea. Write a short story based on this context.', train_dataset_source_path='datasets/english/writingPrompts/train.wp_source', train_dataset_target_path='datasets/english/writingPrompts/train.wp_target', val_dataset_source_path='datasets/english/writingPrompts/valid.wp_source', val_dataset_target_path='datasets/english/writingPrompts/valid.wp_target', test_dataset_source_path='datasets/english/writingPrompts/test.wp_source', test_dataset_target_path='datasets/english/writingPrompts/test.wp_target', source_path='datasets/english/writingPrompts/test.wp_source', target_path='datasets/english/writingPrompts/test.wp_target', field='prompt', per_device_train_batch_size=4, per_device_val_batch_size=1, output_dir='./experiments', max_steps=10000, gradient_accumulation_steps=2, optim='paged_adamw_32bit', learning_rate=0.0002, max_grad_norm=0.3, warmup_ratio=0.03, lr_scheduler_type='constant', group_by_length=True, bnb_4bit_quant_type='nf4', bnb_4bit_compute_dtype='bfloat16', bnb_4bit_use_double_quant=True, gradient_checkpointing=False, lora_alpha=16, lora_dropout=0.1, lora_r=64, bias='none', task_type='CAUSAL_LM', lora_target_modules=['q_proj', 'up_proj', 'o_proj', 'k_proj', 'down_proj', 'gate_proj', 'v_proj'])
1. Loaded dataset.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|█▋        | 1/6 [01:26<07:14, 86.88s/it]Loading checkpoint shards:  33%|███▎      | 2/6 [02:51<05:43, 85.80s/it]