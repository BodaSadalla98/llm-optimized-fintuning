Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.95s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.41s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.64s/it]
Loading the tokenizer from the `special_tokens_map.json` and the `added_tokens.json` will be removed in `transformers 5`,  it is kept for forward compatibility, but it is recommended to update your `tokenizer_config.json` by uploading it again. You will see the new `added_tokens_decoder` attribute that will store the relevant information.
Traceback (most recent call last):
  File "/home/abdelrahman.sadallah/mbzuai/llm-optimized-fintuning/train.py", line 188, in <module>
    trainer.train(args.checkpoint_path)
  File "/home/abdelrahman.sadallah/.conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py", line 1567, in train
    self._load_from_checkpoint(resume_from_checkpoint)
  File "/home/abdelrahman.sadallah/.conda/envs/nlp/lib/python3.10/site-packages/transformers/trainer.py", line 2178, in _load_from_checkpoint
    model.load_adapter(resume_from_checkpoint, model.active_adapter, is_trainable=True)
  File "/home/abdelrahman.sadallah/.conda/envs/nlp/lib/python3.10/site-packages/peft/peft_model.py", line 606, in load_adapter
    load_result = set_peft_model_state_dict(self, adapters_weights, adapter_name=adapter_name)
  File "/home/abdelrahman.sadallah/.conda/envs/nlp/lib/python3.10/site-packages/peft/utils/save_and_load.py", line 158, in set_peft_model_state_dict
    load_result = model.load_state_dict(peft_model_state_dict, strict=False)
  File "/home/abdelrahman.sadallah/.conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py", line 2041, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:
	size mismatch for base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
