Found cached dataset text (/home/daria.kotova/.cache/huggingface/datasets/text/default-ef683339c6e1d1b5/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)
Found cached dataset text (/home/daria.kotova/.cache/huggingface/datasets/text/default-594e1b1157c143e0/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)
Loading cached processed dataset at /home/daria.kotova/.cache/huggingface/datasets/text/default-ef683339c6e1d1b5/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2/cache-f74c4750f7345492.arrow
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [01:31<01:31, 91.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [02:03<00:00, 56.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [02:03<00:00, 61.97s/it]
Traceback (most recent call last):
  File "/home/daria.kotova/ai/llm-optimized-fintuning/calc_metrics.py", line 50, in <module>
    model = PeftModel.from_pretrained(model, args.checkpoint_path)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daria.kotova/miniconda3/lib/python3.11/site-packages/peft/peft_model.py", line 306, in from_pretrained
    model.load_adapter(model_id, adapter_name, is_trainable=is_trainable, **kwargs)
  File "/home/daria.kotova/miniconda3/lib/python3.11/site-packages/peft/peft_model.py", line 606, in load_adapter
    load_result = set_peft_model_state_dict(self, adapters_weights, adapter_name=adapter_name)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daria.kotova/miniconda3/lib/python3.11/site-packages/peft/utils/save_and_load.py", line 158, in set_peft_model_state_dict
    load_result = model.load_state_dict(peft_model_state_dict, strict=False)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/daria.kotova/miniconda3/lib/python3.11/site-packages/torch/nn/modules/module.py", line 2152, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for PeftModelForCausalLM:
	size mismatch for base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
	size mismatch for base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: copying a param with shape torch.Size([1024, 64]) from checkpoint, the shape in current model is torch.Size([4096, 64]).
	size mismatch for base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight: copying a param with shape torch.Size([14336, 64]) from checkpoint, the shape in current model is torch.Size([11008, 64]).
	size mismatch for base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight: copying a param with shape torch.Size([64, 14336]) from checkpoint, the shape in current model is torch.Size([64, 11008]).
/var/lib/slurm-llnl/slurmd/job08651/slurm_script: line 29: --per_device_val_batch_size=1: command not found
