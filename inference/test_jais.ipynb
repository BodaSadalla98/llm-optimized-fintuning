{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel    \n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, BitsAndBytesConfig, StoppingCriteriaList, TextIteratorStreamer\n",
    "from transformers import pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e025932bc0764c5cad7fd41441d1489e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): JAISLMHeadModel(\n",
       "      (transformer): JAISModel(\n",
       "        (wte): Embedding(84992, 5120)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-39): 40 x JAISBlock(\n",
       "            (ln_1): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): JAISAttention(\n",
       "              (c_attn): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=15360, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=15360, bias=True)\n",
       "              )\n",
       "              (c_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=5120, bias=True)\n",
       "              )\n",
       "              (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): JAISMLP(\n",
       "              (c_fc): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=13653, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=13653, bias=True)\n",
       "              )\n",
       "              (c_fc2): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=5120, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=13653, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=5120, out_features=13653, bias=True)\n",
       "              )\n",
       "              (c_proj): Linear4bit(\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.1, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=13653, out_features=64, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=64, out_features=5120, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (base_layer): Linear4bit(in_features=13653, out_features=5120, bias=True)\n",
       "              )\n",
       "              (act): SwiGLUActivation()\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((5120,), eps=1e-05, elementwise_affine=True)\n",
       "        (relative_pe): AlibiPositionEmbeddingLayer()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=5120, out_features=84992, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adapters_name  = \"../experiments/jais/checkpoint-170000\"\n",
    "model_name = \"/home/abdelrahman.sadallah/.cache/huggingface/hub/jais\"\n",
    "\n",
    "# Bits and Bytes config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    # bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    # load_in_4bit=True,\n",
    "\n",
    "    trust_remote_code=True,\n",
    "    device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n",
    "\n",
    "model = PeftModel.from_pretrained(model, adapters_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "orginial = '''\n",
    "دَع عَنكَ لَومي فَإِنَّ اللَومَ إِغراءُ\n",
    "وَداوِني بِالَّتي كانَت هِيَ الداءُ\n",
    "صَفراءُ لا تَنزَلُ الأَحزانُ ساحَتَها\n",
    "لَو مَسَّها حَجَرٌ مَسَّتهُ سَرّاءُ\n",
    "مِن كَفِّ ذاتِ حِرٍ في زِيِّ ذي ذَكَرٍ\n",
    "لَها مُحِبّانِ لوطِيٌّ وَزَنّاءُ\n",
    "قامَت بِإِبريقِها وَاللَيلُ مُعتَكِرٌ\n",
    "فَلاحَ مِن وَجهِها في البَيتِ لَألَاءُ\n",
    "فَأَرسَلَت مِن فَمِ الإِبريقِ صافِيَةً\n",
    "كَأَنَّما أَخذُها بِالعَينِ إِغفاءُ\n",
    "رَقَّت عَنِ الماءِ حَتّى ما يُلائِمُها\n",
    "لَطافَةً وَجَفا عَن شَكلِها الماءُ\n",
    "فَلَو مَزَجتَ بِها نوراً لَمازَجَها\n",
    "حَتّى تَوَلَّدُ أَنوارٌ وَأَضواءُ\n",
    "دارَت عَلى فِتيَةٍ دانَ الزَمانُ لَهُم\n",
    "فَما يُصيبُهُمُ إِلّا بِما شاؤوا\n",
    "لِتِلكَ أَبكي وَلا أَبكي لِمَنزِلَةٍ\n",
    "كانَت تَحُلُّ بِها هِندٌ وَأَسماءُ\n",
    "حاشا لِدُرَّةَ أَن تُبنى الخِيامُ لَها\n",
    "وَأَن تَروحَ عَلَيها الإِبلُ وَالشاءُ\n",
    "فَقُل لِمَن يَدَّعي في العِلمِ فَلسَفَةً\n",
    "حَفِظتَ شَيئاً وَغابَت عَنكَ أَشياءُ\n",
    "لا تَحظُرِ العَفوَ إِن كُنتَ اِمرَأً حَرِجاً\n",
    "فَإِنَّ حَظرَكَهُ في الدينِ إِزراءُ.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "دع عنك لومي فإن اللوم إغراء\n",
      "وداوني بالتي كانت هي الداء\n",
      "صفراء لا تنزل الأحزان ساحتها\n",
      "لو مسها حجر مسته سراء\n",
      "من كف ذات حر في زي ذي ذكر\n",
      "لها محبان لوطي وزناء\n",
      "قامت بإبريقها والليل معتكر\n",
      "فلاح من وجهها في البيت لألاء\n",
      "فأرسلت من فم الإبريق صافية\n",
      "كأنما أخذها بالعين إغفاء\n",
      "رقت عن الماء حتى ما يلائمها\n",
      "لطافة وجفا عن شكلها الماء\n",
      "فلو مزجت بها نورا لمازجها\n",
      "حتى تولد أنوار وأضواء\n",
      "دارت على فتية دان الزمان لهم\n",
      "فما يصيبهم إلا بما شاؤوا\n",
      "لتلك أبكي ولا أبكي لمنزلة\n",
      "كانت تحل بها هند وأسماء\n",
      "حاشا لدرة أن تبنى الخيام لها\n",
      "وأن تروح عليها الإبل والشاء\n",
      "فقل لمن يدعي في العلم فلسفة\n",
      "حفظت شيئا وغابت عنك أشياء\n",
      "لا تحظر العفو إن كنت امرأ حرجا\n",
      "فإن حظركه في الدين إزراء.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyarabic.araby import strip_tashkeel\n",
    "original = strip_tashkeel(orginial)\n",
    "print(original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(model, text: str, tmp=.8, top_p=0.8, max_length=256):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to('cuda')\n",
    "    inputs_length = len(inputs[\"input_ids\"][0])\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_length,\n",
    "                                do_sample=True,\n",
    "                                top_p=top_p,\n",
    "                                temperature=tmp,\n",
    "                                repetition_penalty=1.3\n",
    "                                )\n",
    "\n",
    "    return tokenizer.decode(outputs[0][inputs_length:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "ا, العصر العباسي\n",
      "\n",
      "### Response:\n",
      "دعني فداؤك ما بالغيض من طمع\n",
      "من كان يحسب ان الغيض غضاءا\n",
      "ما كنت احسب أن الغدر عادته\n",
      "حتى رميت بنبل منك حاءى\n",
      "يا رب يوم قد اعرتك مطامعي\n",
      "وما استعرت ولم تستفد رجاءا\n",
      "كنت امرءا لو غدوت البحر لم يكن\n",
      "مهما سواك ولا شفى داءه الشفا\n",
      "فكيف صرت الى خسف وانحطاط ومن\n",
      "خفت عليه ذا جاءك الخافوا\n",
      "لولا هواي لأصبحت في خلقي كما\n",
      "كانت قريش على قوم لهم شرفا\n",
      "أمسكت عن مدحهم حتى كفوك وما\n",
      "في الكف بعد المنع لا الشكر والجزاءا\n",
      "لو عاينتني غداة البين يا أملي\n",
      "والعين تنظرني ذ ذاك ملقى ومثنى\n",
      "لا شك أنك تعرفين الذي صنعت بي\n",
      "يوم الفراق وفي التوديع كيف بكا\n",
      "ولو علمت بأني لست أعطفكم\n",
      "علي يوما لبادلت الوجد جفاءا\n",
      "لكن رأيت زماني فيك مغتربا\n",
      "قد ضل مني سبيله وتاها\n",
      "وأيقنت أني ون طال المدى سنتي\n",
      "لن أحظى بطيف منكم فألقاه\n",
      "الله يعلم اني كلما ذكروا ال\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      ", العصر العباسي\n",
      "\n",
      "### Response:\n",
      "دعني فما لك من لوم على أحد\n",
      "ن كان منك له نفع و صلاحا\n",
      "لا تكذبن فن المرء ليس به\n",
      "من صحة العقل أو زهد نجاحا\n",
      "ولا تكن أبدا في كل ما فعلت\n",
      "ترجو السلامة حتى لا تكون مراحا\n",
      "ما زال مثلك يخشى النقم مذ خلقت\n",
      "وكان غيرك يرجوها طماعا وغراما\n",
      "أعد نظرا ذا لم تدر كيف بدا\n",
      "فنه قد يكون الأمر مستراحا\n",
      "كم حكمة سرت منها وكم فرج\n",
      "قد نال صاحبه ذ جاءه صباحا\n",
      "وكل شيء يرى يبدو لأعيننا\n",
      "لم نخف منه ولا نرجو انصداعا\n",
      "وما رأيت امرءا لاقى الردى فذا\n",
      "يود لو عاش بعد الموت أياما\n",
      "كأنني بك ميتا سوف تتركه\n",
      "في القبر يا ابن أبي بكر فضاجا\n",
      "ونفسه وهو حي راحل معه\n",
      "فن غدا حيث أمسى الناس صرعى\n",
      "وقد مضى عهد أهل الأرض قاطبة\n",
      "فما لنا كلنا يوما بها متاعا\n",
      "هل أنت ممن ترى الدنيا كما ارتحلت\n",
      "وهل تراها لمن يبقى كمتاع\n",
      "لو كنت تعرف فيها بعض رأيك فيه\n",
      "لاستبقيت نفسك الأيام أرسالا\n",
      "يا ليت شعري أأنت اليوم باق أم غدوا\n",
      "على جميع بني العباس طغيانا وملاقا\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      ", العصر العباسي\n",
      "\n",
      "### Response:\n",
      "دعني وما بي من سقمي ومن دائي\n",
      "فانما شفني هم وغماء\n",
      "لا ترج مني شفاء ما حييت ولا\n",
      "ألقى الشفاء ذا شمتت أعداء\n",
      "من لم يذق الموت لا مات من ظمأ\n",
      "فكيف يرجى له يوما دواء\n",
      "قد كنت أحسب أن الناس قد فقدوا\n",
      "في عصرنا الأرض حتى جاءك الماء\n",
      "يا أيها الملك الذي عم نائله\n",
      "أهل البسيطة فضلا ليس نكراءه\n",
      "ن كان في العلم هذا منك معجزة\n",
      "فما لنا فيك يا مولاي داء\n",
      "أنت الطبيب وأنت البحر ملتطم\n",
      "فن أردت شفا فالبحر ماء\n",
      "لم يدر غيرك على الأيام واحدة\n",
      "ولم يكن لك فيها قط رواء\n",
      "حزت العلوم جميعها فبهذه لها\n",
      "هذا وذاك بما أولاك علياء\n",
      "وما أقمت بدار غير بيتك بها\n",
      "ولا حللت سواها وهي أرجاء\n",
      "ولست تنفك عن عز وسؤدد وعلا\n",
      "وكلها بك يوم أنت عطاء\n",
      "لك المام وللأمثال منزلة\n",
      "وعزة ولليث هيبة ونقاء\n",
      "والناس بعدك أشباه وأعيانهم\n",
      "قوم وهم سادة القوم خرقاء\n",
      "كل الملوك ذوي مجد ومفخرة\n",
      "ذا فخرتم بهم أو كنتم سماء\n",
      "ومنزل المجد فيكم كعبة أبدا\n",
      "للعالمين ومحراب وعب\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "ا, العصر العباسي\n",
      "\n",
      "### Response:\n",
      "دعني أداوي التي لا بد منها\n",
      "فانما دواءها النعماء و الرضاءا\n",
      "فالحمد لله حمد الشاكرين له\n",
      "ن الذي يبتغيه المرء قضاءه\n",
      "يا أيها الملك المنصور ن لنا\n",
      "في ملكك اليوم انداد وأشباه\n",
      "لا تخش من حاسدي الدنيا وزخرفها\n",
      "فن ذلك في وجهك الهناءا\n",
      "ولا تكن أبدا يا ابن الأطايب تنثني\n",
      "عن أن تجود على البائسين عطاء\n",
      "قد كان جودكم غيثا عم غايته\n",
      "وكان مجدك فوق النجم علياء\n",
      "وما جرى فيك عن قوم سواك فقد\n",
      "توارثوه قديما فهو أحياء\n",
      "لك كل يوم عدو غير ذي أمل\n",
      "منهم وليس عداك القوم أعداءا\n",
      "لم يستجدوا لك الأعداء بل هم\n",
      "كانوك ما دام فيهم منك احياءا\n",
      "أبناء خير الورى طرا وأكرمهم\n",
      "ونسبتهم أنت للأصهار أبناء\n",
      "لو لم يكن والد الأملاك قد خلقت\n",
      "أيام دولته العليا لكان بقاء\n",
      "أو ليس مالك ذا أبلى الزمان به ال\n",
      "أرض لما بقي فيها ولا ثواءا\n",
      "والناس كلهم مثل الأنام ون\n",
      "عادت لهم نعمة أو عاد شقاء\n",
      "والموت غاية أهل الأرض قاطبة\n",
      "وكل حي لها يوما سيلاقى\n",
      "\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      ", العصر العباسي\n",
      "\n",
      "### Response:\n",
      "دعني فني من الملام بمعزل\n",
      "وأرح فؤادي منك يا عذال\n",
      "ن كان ذنبك أنني رجل أسقمه ال\n",
      "شوق ليك فقد براني السقام\n",
      "أو أن دمعي فوق خدي قد جرى\n",
      "فلأكف عنه فليس فيه ملام\n",
      "أنا مذ رأيت الشمس في مغربها لم\n",
      "تلق على وجه البسيطة ظلاما\n",
      "يا بدر لا تكتم سراك فنني\n",
      "ما زلت فيك متيما مستهاما\n",
      "فالعيس تستبق الرياح ذا سرت\n",
      "والليل يتبعها ويسحب ذيلا\n",
      "والماء يسابق ن مر النسيم به\n",
      "ويصبغ الليل النهار قتاما\n",
      "لا تنكرنك سوابق الدمع التي\n",
      "لم يبق فيها اليوم شيئا حراما\n",
      "وتخوف الناس الجفاء فلم أكن\n",
      "لوصلهم لو جفوك يوما لجافا\n",
      "وذا نظرت لى هواي وجدتهم\n",
      "في كل يوم يستزيدون غراما\n",
      "ولقد عهدتك فيهم واحدا فردا فعاد\n",
      "لى التثليث بينهم فتسامى\n",
      "هل أنت ممن يعبد الأصنام أو\n",
      "يشرك بالله ولا يتسامى\n",
      "من بعد ما شاهدت حسن وجوههم\n",
      "وبقيت أنظرها وأجحدها حالا\n",
      "ولربما غنت عليهم حناجر\n",
      "وسعى بهم عود ورنح قوام\n",
      "وتراقصت معهم يد طرب كما\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      ", العصر الحديث\n",
      "\n",
      "### Response:\n",
      "دع عنك لومي ف اللوم اغضاء  \n",
      "داويني بالتي كانت هي الدواءا\n",
      "سقمي من سقامك ما شفى جسدي\n",
      "حتى براني السقم و انت شفاءى\n",
      "لا تسقني دواء منك غيرك انه\n",
      "ما لي سوى الله ثم انا الضعفاءا\n",
      "يا نفس يا نفس لو تدرين منزلة\n",
      "لغير حبك لم تحبك السماءا\n",
      "من كان في الارض لا يحظى بجنة\n",
      "الا اذا شاءها المولى جزاءا\n",
      "لم يطع الامر الا الذي عصت به ال\n",
      "شياطين اذ امره قد باء عظاءا\n",
      "وما على ارضنا خلق له بشر\n",
      "لو ارتضاه لبانوا فوق الظناءا\n",
      "ولكل قوم اله يعبدونه فلا\n",
      "يرضى بغيرهم مولى ولا سماءا\n",
      "والناس شتى ولكن كل امة ترى\n",
      "في غير معتقدها ذاك الشقاءا\n",
      "فليس يدري بما يجنيه ذو حسب\n",
      "ولا كريم ولا حر وشيءءا\n",
      "وقد يكون الفتى حرا كريما ومن يكن\n",
      "حرا فقد يتعب الحر والعلاءا\n",
      "قد يرتقى المرء حتى يعلو مراتب\n",
      "وهو الى الموت ن عاش الرعاءءا\n",
      "مالي ارى الناس طرا بين غافل\n",
      "عن دينه او متبع اهواءا\n",
      "وكل امرىء\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "ا\n",
      "لا تسقني سم لوم كنت احمله\n",
      "فالله يعلم اني لست انسى النائيا\n",
      "يا صاح كم قد جرى في النفس من سقمي\n",
      "ولم اكن قط الا عاشقا دائيا\n",
      "فكم لهذي الليالي وهي نائمة\n",
      "توقظن قلبا الى الاحزان ذا راءى\n",
      "وكم لها بين قوم لم يكونوا ذوي الالفة او قرابة اذ رأت بهم هوى فتناءوا وان شأوا الوصال تناءت عنهم ولم يجدوا فيها لهم مرءاة ولا مشبها ابدا يا خليلي لا تلمني فما ذنبي اذا ما عشت مغرما صبوة ولوعا وهواها وراعيت عهد ودادها وصنها عن صروف الزمان فانها ان تقضت غدا تجد غيري لها حبيبا وما كان ذاك الحبيب الذي مضى لي بها يوما سوى الطيف يسري والليل معتكر الضياءا\n",
      "\n",
      "وما على المرء عند الموت لو انقضى اجرا\n",
      "ان ليس يجزى بنسان وناسا\n",
      "او انه غير الدنيا التي سكنت\n",
      "فيها ولو شاء لاستبدلها بخراه\n",
      "لو انها جنة الخلد الحياة لمن\n",
      "كان عليها لما اختار البقاء\n",
      "لكننا لسنا ندري بعد ماذا عسى\n",
      "من غد سوف يبقى فيه باقيا \n",
      "وانها لنار كلما اتقدت\n",
      "كانت لنا منها نار ذات ضوضاءا\n",
      "هل ينتهي الدهر والدنيا ك\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "ا\n",
      "يا عاذلي لا تلمني ان صددت فما\n",
      "يلوم الا امرء قد ضل اهداءا\n",
      "فاذا سمعت حديث العاشقين فلا\n",
      "تلمنهم ن رأوا في الحب ما اقصاهوا\n",
      "ان المحب اذا هام عشقا لم يجد\n",
      "عذلا يلوم به من كان ملتهى\n",
      "او رام سلوان وجد القلب مضطربا\n",
      "يريد صبرا ولكن ليس يجده\n",
      "ما ضره لو دنا او شاء مرتحله\n",
      "لعلني اشفى منه علتي برؤياه\n",
      "ن كنت اسلو فقد غدت سجاياه لي ارثا\n",
      "لم انس اذ زار ليلا بعد ليلتنا الالى\n",
      "وقد بدا بدر التمام بغرته السنى\n",
      "وافتر عن در ثغور طاب ريقها\n",
      "والثغر يندى بثغري ذ سما البينا\n",
      "وعاد يعتنق المشتاق محتضنه\n",
      "شوقا كما عاد ذاك الصدغ معتلى الهنا\n",
      "القى له خدا على خد يمانيا\n",
      "واصبحت ابصر فيه النور مبتسما\n",
      "حتى ظننت بان البدر بات هو انا\n",
      "قد قال قوم بانه متلون لكن\n",
      "ليس التلون يوما طبع اهل الهوى\n",
      "هل ينكروا حب ظبي غير لونه ام\n",
      "ينكرون الوفا والحب واليمانا\n",
      "اصفو\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "ا, العصر الجاهلي\n",
      "\n",
      "### Response:\n",
      "يا من حوى فضل الحسان و ن يكن\n",
      "قد ضل في طرقه فاللوم داءا\n",
      "ان كان عندك علم بالذي صنعت يدي\n",
      "فاخبر فديتك ما لي فيك سلوانا\n",
      "لا تنكروا ان رمت وصلك بعدما\n",
      "اصبحتم للنسوان فيه فتانا\n",
      "لو لم أكن طوع الهوى ما كنت قد\n",
      "أضعته بعد النحول بدائيا\n",
      "ولكنت اطمع أن تعود لى كما\n",
      "كنا عليه ولكن ليس رجعى كانا\n",
      "ما ذقت بعدك غير نار صبوتي\n",
      "ذ لا أراها وان طالت نيرانا\n",
      "والناس لولاهم لكانوا مثل الذي\n",
      "لم يدر طعم الحب حتى عرف الضنا\n",
      "من رام منهم بالغرام وسيلة\n",
      "يوفي بها يوما فلا يخدعنك أنا\n",
      "كم طالب الوصل الجميل فلم أطق\n",
      "وصال ذات الخال والورد الجني\n",
      "وكم سقيت مدامة منها على\n",
      "أنهارها عذبات البان أغصانا\n",
      "ورضيت بالأشواق والشوق الرضى\n",
      "ولو استطعت لما رضينا البعادا\n",
      "وأطعت قلبي حين قال لو انه\n",
      "بانت عن الألاف اصبحن أشجانا\n",
      "عيني جفا نومها الدمعة التي\n",
      "تنهل والقلب غدا له خفق\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 31\u001b[0m\n\u001b[1;32m     26\u001b[0m f\u001b[39m.\u001b[39mwrite(text \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m f\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m20\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 31\u001b[0m output \u001b[39m=\u001b[39m gen(model, text)\n\u001b[1;32m     33\u001b[0m output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m<newline>\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     35\u001b[0m f\u001b[39m.\u001b[39mwrite(output \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m, in \u001b[0;36mgen\u001b[0;34m(model, text, tmp, top_p, max_length)\u001b[0m\n\u001b[1;32m      3\u001b[0m inputs_length \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(inputs[\u001b[39m\"\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m])\n\u001b[1;32m      4\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode():\n\u001b[0;32m----> 5\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs, max_new_tokens\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m      6\u001b[0m                             do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m      7\u001b[0m                             top_p\u001b[39m=\u001b[39;49mtop_p,\n\u001b[1;32m      8\u001b[0m                             temperature\u001b[39m=\u001b[39;49mtmp,\n\u001b[1;32m      9\u001b[0m                             repetition_penalty\u001b[39m=\u001b[39;49m\u001b[39m1.3\u001b[39;49m\n\u001b[1;32m     10\u001b[0m                             )\n\u001b[1;32m     12\u001b[0m \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mdecode(outputs[\u001b[39m0\u001b[39m][inputs_length:], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/peft/peft_model.py:1034\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model\u001b[39m.\u001b[39mgeneration_config \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneration_config\n\u001b[1;32m   1033\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1034\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model\u001b[39m.\u001b[39;49mgenerate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1035\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[1;32m   1036\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model\u001b[39m.\u001b[39mprepare_inputs_for_generation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model_prepare_inputs_for_generation\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/generation/utils.py:1799\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1791\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1792\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1793\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1794\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1795\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1796\u001b[0m     )\n\u001b[1;32m   1798\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1799\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1800\u001b[0m         input_ids,\n\u001b[1;32m   1801\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1802\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1803\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1804\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1805\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1806\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1807\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1808\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1809\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1810\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1811\u001b[0m     )\n\u001b[1;32m   1813\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1814\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1815\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1816\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1817\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1822\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   1823\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/transformers/generation/utils.py:2896\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2893\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2895\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2896\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2897\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2898\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2899\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2900\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2901\u001b[0m )\n\u001b[1;32m   2903\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2904\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/jais/modeling_jais.py:1115\u001b[0m, in \u001b[0;36mJAISLMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1107\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m \u001b[39m    Labels for language modeling. Note that the labels **are shifted** inside the model, i.e. you can set\u001b[39;00m\n\u001b[1;32m   1110\u001b[0m \u001b[39m    `labels = input_ids` Indices are selected in `[-100, 0, ..., config.vocab_size]` All labels set to `-100`\u001b[39;00m\n\u001b[1;32m   1111\u001b[0m \u001b[39m    are ignored (masked), the loss is only computed for labels in `[0, ..., config.vocab_size]`\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1115\u001b[0m transformer_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransformer(\n\u001b[1;32m   1116\u001b[0m     input_ids,\n\u001b[1;32m   1117\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1118\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   1119\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m   1120\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m   1121\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1122\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   1123\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1124\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   1125\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1126\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1127\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1128\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1129\u001b[0m )\n\u001b[1;32m   1130\u001b[0m hidden_states \u001b[39m=\u001b[39m transformer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1132\u001b[0m \u001b[39m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/jais/modeling_jais.py:936\u001b[0m, in \u001b[0;36mJAISModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    926\u001b[0m     outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    927\u001b[0m         create_custom_forward(block),\n\u001b[1;32m    928\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    933\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    934\u001b[0m     )\n\u001b[1;32m    935\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 936\u001b[0m     outputs \u001b[39m=\u001b[39m block(\n\u001b[1;32m    937\u001b[0m         hidden_states,\n\u001b[1;32m    938\u001b[0m         layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    939\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    940\u001b[0m         head_mask\u001b[39m=\u001b[39;49mhead_mask[i],\n\u001b[1;32m    941\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    942\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    943\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    944\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    945\u001b[0m         position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m    946\u001b[0m     )\n\u001b[1;32m    948\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    949\u001b[0m \u001b[39mif\u001b[39;00m use_cache \u001b[39mis\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/jais/modeling_jais.py:441\u001b[0m, in \u001b[0;36mJAISBlock.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, position_bias)\u001b[0m\n\u001b[1;32m    439\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    440\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln_1(hidden_states)\n\u001b[0;32m--> 441\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(\n\u001b[1;32m    442\u001b[0m     hidden_states,\n\u001b[1;32m    443\u001b[0m     layer_past\u001b[39m=\u001b[39;49mlayer_past,\n\u001b[1;32m    444\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    445\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m    446\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    447\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    448\u001b[0m     position_bias\u001b[39m=\u001b[39;49mposition_bias,\n\u001b[1;32m    449\u001b[0m )\n\u001b[1;32m    450\u001b[0m attn_output \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]  \u001b[39m# output_attn: a, present, (attentions)\u001b[39;00m\n\u001b[1;32m    451\u001b[0m outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m1\u001b[39m:]\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/jais/modeling_jais.py:380\u001b[0m, in \u001b[0;36mJAISAttention.forward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, position_bias)\u001b[0m\n\u001b[1;32m    377\u001b[0m     attn_output, attn_weights \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_attn(query, key, value, attention_mask, head_mask, position_bias)\n\u001b[1;32m    379\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_merge_heads(attn_output, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim)\n\u001b[0;32m--> 380\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mc_proj(attn_output)\n\u001b[1;32m    381\u001b[0m attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresid_dropout(attn_output)\n\u001b[1;32m    383\u001b[0m outputs \u001b[39m=\u001b[39m (attn_output, present)\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:290\u001b[0m, in \u001b[0;36mLinear4bit.forward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     expected_dtype \u001b[39m=\u001b[39m result\u001b[39m.\u001b[39mdtype\n\u001b[1;32m    288\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(lora_A\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m--> 290\u001b[0m output \u001b[39m=\u001b[39m lora_B(lora_A(dropout(x)))\n\u001b[1;32m    291\u001b[0m \u001b[39mif\u001b[39;00m requires_conversion:\n\u001b[1;32m    292\u001b[0m     output \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mto(expected_dtype)\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/linear.py:113\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    110\u001b[0m         bound \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m math\u001b[39m.\u001b[39msqrt(fan_in) \u001b[39mif\u001b[39;00m fan_in \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m    111\u001b[0m         init\u001b[39m.\u001b[39muniform_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias, \u001b[39m-\u001b[39mbound, bound)\n\u001b[0;32m--> 113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mlinear(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias)\n\u001b[1;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mextra_repr\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "base_text = '''### Instruction: Generate a poem based on the following title, and the given era:\n",
    "\n",
    "### Input:\n",
    "'''\n",
    "\n",
    "text = '''دع عنك لومي فان اللوم اغراء \n",
    "و داوني بالتي كانت هي الداء'''\n",
    "\n",
    "# text = ''' أصبح الملك للذي فطر الخلق , العصر العثماني'''\n",
    "\n",
    "# base_text = ''''''\n",
    "\n",
    "# Happy kid was playing at the park, but then he broke his leg, and his life got completely changed.\n",
    "\n",
    "with open('../results/arabic_output.txt', 'a') as f:\n",
    "    while 1:\n",
    "        print('-'*20)\n",
    "        # text = input(\"Enter a prompt: \")\n",
    "        print('-'*20)\n",
    "\n",
    "        text = base_text + text\n",
    "\n",
    "        # text = 'who are you?'\n",
    "        \n",
    "        f.write('='*30 + '\\n')\n",
    "        f.write(text + '\\n')\n",
    "        f.write('-'*20 + '\\n')\n",
    "\n",
    "\n",
    "    \n",
    "        output = gen(model, text)\n",
    "\n",
    "        output = output.replace('<newline>', '\\n')\n",
    "\n",
    "        f.write(output + '\\n')\n",
    "        f.write('='*30 + '\\n')\n",
    "        f.flush()\n",
    "\n",
    "        print('-'*20)\n",
    "        print(output)\n",
    "        \n",
    "        print('-'*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = get_arabic_datasets(field = 'prompt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction: Generate a poem based on the following title, and the given era:\n",
      "\n",
      "    ### Input:\n",
      "    أصبح الملك للذي فطر الخلق , العصر العثماني\n",
      "\n",
      "    ### Response:\n",
      "    أصبح الملك للذي فطر الخل\n",
      "ق بتقدير للعزيز العليم\n",
      "غافر الذنب للمسيء بعفو\n",
      "قابل التوب ذي العطاء العميم\n",
      "مرسل المصطفى البشير لينا\n",
      "رحمة منه بالكلام القديم\n",
      "ربنا ربنا ليك أنينا\n",
      "فأجرنا من حر نار الجحيم\n",
      "واكفنا شر ما نخاف بلطف\n",
      "يا عظيما يرجى لكل عظيم\n",
      "وتقبل أعمالنا واعف عنا\n",
      "وأنلنا دخول دار النعيم\n",
      "بنبي بعثته فهدانا\n",
      "لصراط من الهدى مستقيم\n",
      "وبمن نحن في حماه مدى الدهر\n",
      "أخيه يحيى الحصور الكريم\n",
      "أدرك أدرك قوما أتوا بافتقار\n",
      "وانكسار ومدمع مسجوم\n",
      "شهدت أرواحهم أنك الله\n",
      "وجاءوا بكل قلب سليم\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0]['prompt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling `cublasGemmEx( handle, opa, opb, m, n, k, &falpha, a, CUDA_R_16F, lda, b, CUDA_R_16F, ldb, &fbeta, c, CUDA_R_16F, ldc, CUDA_R_32F, CUBLAS_GEMM_DFALT_TENSOR_OP)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m x \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCan you tell me who are you\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m inputs \u001b[39m=\u001b[39m tokenizer(x, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m out \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs, return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/jais/modeling_jais.py:1137\u001b[0m, in \u001b[0;36mJAISLMHeadModel.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1134\u001b[0m     torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mset_device(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mfirst_device)\n\u001b[1;32m   1135\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1137\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlm_head(hidden_states)\n\u001b[1;32m   1138\u001b[0m lm_logits \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\n\u001b[1;32m   1139\u001b[0m     \u001b[39mfloat\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_logits_scale), dtype\u001b[39m=\u001b[39mlm_logits\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39mlm_logits\u001b[39m.\u001b[39mdevice\n\u001b[1;32m   1140\u001b[0m )\n\u001b[1;32m   1142\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/accelerate/hooks.py:164\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39m_old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    163\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 164\u001b[0m     output \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/.conda/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: CUBLAS_STATUS_INTERNAL_ERROR when calling `cublasGemmEx( handle, opa, opb, m, n, k, &falpha, a, CUDA_R_16F, lda, b, CUDA_R_16F, ldb, &fbeta, c, CUDA_R_16F, ldc, CUDA_R_32F, CUBLAS_GEMM_DFALT_TENSOR_OP)`"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# x = train_dataset[0]['prompt']\n",
    "x = \"Can you tell me who are you\"\n",
    "\n",
    "inputs = tokenizer(x, return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "out = model(**inputs, return_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'t tell me what you you?\"\n",
      "\n",
      "Can you tell me who are you\n"
     ]
    }
   ],
   "source": [
    "labels =  torch.argmax(out.logits, dim=-1)\n",
    "\n",
    "l = tokenizer.decode(labels[0])\n",
    "\n",
    "print(l)\n",
    "\n",
    "print(tokenizer.decode(inputs['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
